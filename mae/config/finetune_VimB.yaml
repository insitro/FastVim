accum_iter: 1
batch_size: 128  # make sure it is overall 1024
num_workers: 24
pretrained_checkpoint_path: '/path/to/ckpt'  # provide path to pretrained checkpoint from MAE/SSL
num_nodes: 1

pl_seed: 0

img_size: 224
patch_size: 16 # for now only implemented when both stride and patch size is same

use_norm_after_ssm: True
blr: 0.0005
layer_decay: 0.65
training_epochs: 100
gradient_clip_val: -1 # -1 flag mean don't use gradient clipping

weight_decay: 0.05
drop_path_rate: 0.3
min_lr: 0.00001
warmup_epochs: 5
num_classes: 1000

vim_config:
  _target_: models.fastvim.vim_base_patch16_224_final_pool_mean_abs_pos_embed_with_midclstok_div2
  pretrained: false
  img_size: ${img_size}
  num_classes: ${num_classes}
  drop_path_rate: ${drop_path_rate}
  channels: 3
  patch_size: ${patch_size}
  stride: ${patch_size}
  use_norm_after_ssm: ${use_norm_after_ssm}
  pretrained_checkpoint_path: ${pretrained_checkpoint_path}


# Configuration for the dataloader
data_config:
  _target_: mae.datasets_finetune.load_DataModule
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  #augmentation
  img_size: ${img_size}
  hflip: 0.5
  vflip: 0.0
  eval_crop_ratio: 0.875
  color_jitter: 0.0  # only randaug in fine tuning according to MAE
  auto_augment: 'rand-m9-mstd0.5-inc1'
  interpolation: 'bicubic'
  re_prob: 0.25
  re_mode: 'pixel'
  re_count: 1


model_config:
  _target_: mae.finetune_imagenet.SupervisedModule
  backbone: ${vim_config}
  num_classes: ${num_classes}
  label_smoothing: 0.1
  mixup: 0.8
  cutmix: 1.0
  mixup_mode: 'batch'
  mixup_prob: 1.0
  mixup_switch_prob: 0.5
  weight_decay: ${weight_decay}
  blr: ${blr}
  batch_size: ${batch_size}
  warmup_epochs: ${warmup_epochs}
  scheduling_epochs: ${training_epochs}
  min_lr: ${min_lr}
  layer_decay: ${layer_decay}
  accum_iter: ${accum_iter}
